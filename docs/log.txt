-----------------------------------3-17-2020------------------------------------

Better late than never to log notes.
Currently have written most logic for functions, as well as dividing project
into multiple files. I am now commencing testing on certian functions that
don't depend on too many others.

First is the getSentenceListFromFiles() function, I need to figure out how to
properly parse words and split them in half if they have a punctuation that
doesn't terminate a sentence. I created a function to bring a string to
lowercase since that is also required.

I'm noting that parsing the text in C++ can be quite long winded. It's probably 
less of a hassle on Python, the intended language for this project. Perhaps 
consider including the text parsing as starter code.

This push successfully read the test-hows-it-going.txt file into a 2D vector,
dividing each sentence into a vector of strings.


-----------------------------------3-19-2020------------------------------------

Now I'm adapting the getSentenceListFromFiles() function to the
getSentenceLists() function. I might consider changing parameters for
getSentenceLists() to stringstream instead of string.

Test main() runs perfectly, so it seems the functions work together properly.

I also added the pdf handout so that I don't have to keep crossing glances over
at my pdf viewer.

-----------------------------------3-20-2020------------------------------------

It appears buildSemanticDescriptors() does its job quite well, and my test files
outputted what the examples from the handout describe (see "I am a sick man..."
bottom middle of first page). I figure the trickiest part is done. Now all that
is needed is to use the semantic description with the proper math to calculate
word similarites.

I moved the filenames from a hard encoded vector in main() to a file
book-names.txt that is read from a helper function, getFiles(). That way I can 
add or remove books without recompiling. 

Displaying the semantic description of a full book shows I still need to work on
tokenizing, still a few words have quotes around them. It's worth the trouble
considering double quotes are in like every book ever. Again, working with
strings is feeling a bit tedious.

-----------------------------------3-28-2020------------------------------------

Program is functional!

First I had to rewrite the two provided funcitons norm() and cosineSimilarity(),
because while the function descriptions described the parameters as vectors, the
reality was that the functions would need maps, not vectors, to pass along. So I
adjusted vector syntax to map syntax accordingly. The maps were the semantic
descriptors of words, which is only referred to as a vector to fit the
mathematical use of cosine similarity and dot product. Typically math and
computer science are not so confusing together.

The functions themselves were not too bad considering that's where all the magic
happens. Typical use of iterators are all around the program.

Following the instruction outlined in the handout (./handout.pdf), I parsed two
books, Swann's Way and War and Peace. For prompts I used the 40 questions
provided on the NIFTY page (../cpp/test.txt). My accuracy came to about 74%,
however I noticed that is despite the fact that at least 7 words prompted did
not exist in the semantic dictionary (therefore they were not read in any of the
books). Consider providing test synonym problems that have words that actually
exist in the books that are read in. I.e. if students are given a word and two
possible synonyms, all three words should be in the books parsed, or else they
won't exist in the semantic description and the computer may fail.

I didn't bother changing the string tokenizing too much. I am following the
intermediate instructions which omit delimiting with parentheses, and other
restrictions. The logic is that there will be an insignificant amount of 
incorrectly read words in the semantic dictionary. Perhaps it might be worth
polishing the tokenizing.

One choice I have yet to make is whether I should remove a word's duplicate.
That is, the semantic descriptor for the word "it" includes the word "it", and
I hypothesize a word's duplicate semantic descriptor has a high value
(repetition is common).

There's a few more things I want to do before I am satisifed:

1. Rewrite function parameters to be constant references when possible to save
   space.
2. Rewrite loops to iterator colon syntax (if possible)
3. Insert clarifications of code where it can help.
4. Get feedback on code, runtime.

And for my own interest:

1. Rewrite in Python
2. Add books to increase dataset. 
